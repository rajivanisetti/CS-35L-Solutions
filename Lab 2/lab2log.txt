First, I set my locale to the standard C locale, using the following command: export LC_ALL="C".

Then, I sorted the file specified in the assignment page and placed the sorted file in my working directory using the following command: sort /usr/share/dict/words > words

After this, I downloaded the html for the assignment page using wget in the following command: wget http://web.cs.ucla.edu/classes/winter18/cs35L/assign/assign2.html

I then switched the contents of this to a text file, using the following mv command.
mv assign2.html assign2.txt

I used the following command, as the assignment page stated: tr -c 'A-Za-z' '[\n*]' < assign2.txt

I noticed that this command essentially split up each of the words (I presume that contain characters from A-Z or a-z), and put many newlines between them, the number of newlines varied. It seems that the number of newlines was correlated with the non-A-Z and non-a-z characters, regarding how many characters and which characters they were. All in all, it seems the command split up the text file into only english alphabet characters and issued newlines depending on all other characters.

I then used the following command, as the assignment page stated: tr -cs 'A-Za-z' '[\n*]' < assign2.txt

It seems that this command did the same as the previous command. However, in this case the command deleted all of the surplus newlines and put each string of English alphabet on its own line without any surplus whitespace after. 

I then used the following command, as the assignment page stated: tr -cs 'A-Za-z' '[\n*]' < assign2.txt | sort 

The reason that I put the "sort" keyword after is so that the command has a file to sort. If I were to put "< assign2.txt" at the end as I did with the previous commands, the sort would not sort as intended. I noticed this because I did it incorrectly the first time, and fixed it. 

It seems that this output sorted the previous output, so that we can see the file with all duplicates in sorted order. In essence, this command takes all English letters and words and puts them on a new line in sorted order. 

After this, I invoked the next command in the same manner: tr -cs 'A-Za-z' '[\n*]' < assign2.txt | sort -u

The difference between the output of this command and the previous is that this command used the "unique" -u option and took out all duplicates. From this, we have a list of every English word or letter in the html document in sorted order without any duplicates. 

I then tried the following command to compare its output: tr -cs 'A-Za-z' '[\n*]' < assign2.txt | sort -u | comm - words

This command outputted a plethora of words. After doing some research on the "comm" command, I realized that it printed three "columns", the first column was lines unique standard input, but for us this is the file given by the command tr -cs 'A-Za-z' '[\n*]' < assign2.txt | sort -u. The second column was lines unique to the file "words", and the third column was lines that were common to both files. 

I then tried the following command to compare its output: tr -cs 'A-Za-z' '[\n*]' < assign2.txt | sort -u | comm -23 - words

This command only outputted lines unique to the file given by the command tr -cs 'A-Za-z' '[\n*]' < assign2.txt | sort -u, or standard input. This is because the flag -23 supresses the second and third columns. 

I then started the portion of the lab with regards to the Hawaiian dictionary. First, I used wget http://mauimapp.com/moolelo/hwnwdseng.htm to download the html file. I then used emacs to start writing my bash script.

I started off my script with the trademark shebag: #!/bin/bash

The following is my bash script with comments explaining each command.

#!/bin/bash                                                                                                                       

# My bash script                                                                                                                  

#Extract all table entries                                                                                                        

grep '<td>.\{1,\}</td>' |

#Delete all odd entries                                                                                                       

sed -n '2~2p' |

#Delete all <tr> and </tr> tagging around words                                                                               

sed 's/<td>//g;s/<\/td>//g' |

#Delete all leading spaces                                                                                                    

sed 's/^\s*//g' |

#Convert all uppercase characters to lowercase                                                                                

tr [:upper:] [:lower:] |

#Delete all <u> and </u> tags                                                                                                 

sed 's/<u>//g;s/<\/u>//g' |

#Treat all ` as if it is a '                                                                                                  

sed "s/\`/\'/g" |

#Treat a comma and then a space between words or a space between words as 2 words and replace them with newlines 

sed "s/,\s/\n/g;s/\s/\n/g" |

#Take out any invalid Hawaiian words without the special Hawaiian characters                                                  

grep "^[pk\'mnwlhaeiou]\{1,\}$" |

#Finally, sort and remove duplicates                                                                                          

sort -u

#end of bash script

--------------------------------------------------

After this, I put the script's output into a file called hwords using the command cat hwnwdseng.htm | ./buildwords > hwords

I used the following command to check how many words on the assignment page were mispelled in Hawaiian:

tr [:upper:] [:lower:] < assign2.txt | tr -cs "pk\'mnwlhaeiou" "[\n*]" | sort -u | comm -23 - hwords | wc -l

It output the number 198, so there were 198 mispelled Hawaiian words on the assignment page. 

In terms of English mispellings, I used the following command to check:

tr [:upper:] [:lower:] < assign2.txt | tr -cs 'A-Za-z' '[\n*]' | sort -u | comm -23 - words | wc -l

This command output 39, so there were 39 mispelled English words. 

I used the following command to put those mispelled English words in a file named misspelledEnglish:

tr [:upper:] [:lower:] < assign2.txt | tr -cs 'A-Za-z' '[\n*]' | sort -u | comm -23 - words > misspellEnglish

After this, I used the following command to find words mispelled in English but not Hawaiian.

tr [:upper:] [:lower:] < misspellEnglish | tr -cs "pk\'mnwlhaeiou" "[\n*]" | sort -u | comm -12 - hwords

Here are the words:

e
halau
i
lau
po
wiki

I then used the following command to put mispelled Hawaiian words into a file name misspelledHawaiian:

tr [:upper:] [:lower:] < assign2.txt | tr -cs "pk\'mnwlhaeiou" "[\n*]" | sort -u | comm -23 - hwords > misspelledHawaiian

I then used the following command to find words mispelled in Hawaiian but not English:

tr [:upper:] [:lower:] < misspelledHawaiian | tr -cs 'A-Za-z' '[\n*]' | sort -u | comm -12 - words

The words are:

a
ail
ain
ake
al
ale
alen
all
amine
amp
ample
an
aph
awk
e
ea
ee
el
em
emp
en
ep
epa
h
ha
han
hap
hawaiian
he
hei
hell
hem
hen
hi
hin
ho
how
howe
ia
ie
ii
ile
imp
in
ion
iou
k
keep
kin
l
lan
le
lea
li
like
line
link
ln
lo
lowe
m
mail
man
me
men
mi
ml
mo
mp
n
name
ne
nee
no
non
nu
num
o
om
on
one
op
ope
open
own
p
paul
pe
pell
people
plea
pu
u
ui
ula
ule
ume
ump
un
uni
w
wa
wan
we
wh
wha
who
wi
wo















